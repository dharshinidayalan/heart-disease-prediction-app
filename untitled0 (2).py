# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ADlswlS2E_MYQwas-gh2L-ICqGR1xRIa
"""

import sklearn
import pandas as pd
import numpy as np
print("sklearn:", sklearn.__version__)
print("pandas:", pd.__version__)
print("numpy:", np.__version__)

"""IMPORT  LIBRARIES"""

import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from imblearn.over_sampling import SMOTE
from sklearn.utils import shuffle
import matplotlib.pyplot as plt
import seaborn as sns

"""LOAD DATASET"""

# Load dataset
df = pd.read_csv('hd dataset.csv')
print("Dataset loaded successfully.")

print('Shape of the data is ', df.shape)

df

df.info()

df.head()

df.dtypes

"""DATA PREPROCESSSING"""

#checking missing values

print(df.isnull().sum())

#remove duplicates

print(df.duplicated().sum())

df = df.drop_duplicates()
df.shape

# Ensure Reproducibility
np.random.seed(42)
os.environ['PYTHONHASHSEED'] = '42'

# Convert 'Target' to numerical if it's categorical

if df['Target'].dtype == 'object':
    le = LabelEncoder()
    df['Target'] = le.fit_transform(df['Target'])
    print("Target column encoded.")

# Feature Engineering

df['Health_Score'] = df['PhysicalHealth'] + df['MentalHealth']
age_mapping = {
    '18-24': 1, '25-29': 2, '30-34': 3, '35-39': 4, '40-44': 5,
    '45-49': 6, '50-54': 7, '55-59': 8, '60-64': 9, '65-69': 10,
    '70-74': 11, '75-79': 12, '80 or older': 13
}
df['AgeCategory'] = df['AgeCategory'].map(age_mapping)
print("Feature engineering completed.")

# Identify categorical and numerical columns
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
numerical_cols.remove('Target')  # Exclude target from numerical processing

"""Plot Distribution of Numerical Features"""

# Define custom color palette
mypal = ['#FC05FB', '#FEAEFE', '#FCD2FC', '#F3FEFA', '#B4FFE4', '#3FFEBA']

# Identify numerical features before selection
num_feats = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Grid setup for subplots
L = len(num_feats)
ncol = 2
nrow = int(np.ceil(L / ncol))

# Create Figure
fig, ax = plt.subplots(nrow, ncol, figsize=(16, 14), facecolor='#F6F5F4')
fig.subplots_adjust(top=0.92)

i = 1
for col in num_feats:
    plt.subplot(nrow, ncol, i, facecolor='#F6F5F4')

    # KDE Plot for numerical features before transformation
    ax = sns.kdeplot(data=df, x=col, hue="Target", multiple="stack", palette=mypal[1::4])
    ax.set_xlabel(col, fontsize=16)
    ax.set_ylabel("Density", fontsize=16)

    sns.despine(right=True)
    sns.despine(offset=0, trim=False)

    i += 1

plt.suptitle('Distribution of Numerical Features (Before Selection & Conversion)', fontsize=24)
plt.show()

""" Plot Distribution of Categorical Features"""

# Identify categorical features before transformation
cat_feats = df.select_dtypes(include=['object', 'category']).columns.tolist()

# Grid setup for subplots
L = len(cat_feats)
ncol = 2
nrow = int(np.ceil(L / ncol))

# Create Figure
fig, ax = plt.subplots(nrow, ncol, figsize=(16, 14), facecolor='#F6F5F4')
fig.subplots_adjust(top=0.92)

i = 1
for col in cat_feats:
    plt.subplot(nrow, ncol, i, facecolor='#F6F5F4')

    # Countplot for categorical features before encoding
    ax = sns.countplot(data=df, x=col, hue="Target", palette=mypal[1::4])

    # Add value labels
    for p in ax.patches:
        height = p.get_height()
        ax.text(p.get_x() + p.get_width()/2., height + 3, '{:1.0f}'.format(height), ha="center",
                bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.5))

    ax.set_xlabel(col, fontsize=16)
    ax.set_ylabel("Count", fontsize=16)

    sns.despine(right=True)
    sns.despine(offset=0, trim=False)

    i += 1

plt.suptitle('Distribution of Categorical Features (Before One-Hot Encoding)', fontsize=24)
plt.show()

# Convert categorical variables into numerical using One-Hot Encoding
encoder = OneHotEncoder(drop='first', sparse_output=False)
categorical_encoded = encoder.fit_transform(df[categorical_cols])
categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(categorical_cols))
print("One-Hot Encoding completed.")

categorical_encoded_df = categorical_encoded_df.astype('float32')
print(categorical_encoded_df.dtypes)

# Normalize numerical features using Z-score normalization
scaler = StandardScaler()
numerical_scaled = scaler.fit_transform(df[numerical_cols])
numerical_scaled_df = pd.DataFrame(numerical_scaled, columns=numerical_cols)
print("Feature scaling completed.")

print(numerical_scaled_df.head())  # Display first few rows
print(numerical_scaled_df.dtypes)

# Combine processed numerical and categorical features
df_processed = pd.concat([numerical_scaled_df, categorical_encoded_df], axis=1)
print("Data preprocessing completed. Columns in df_processed:", df_processed.columns.tolist())

df_processed.dtypes

"""FEATURE SELECTION"""

# Apply PCA
pca = PCA(n_components=10, random_state=42)  # Select top 7 principal components
X_pca = pca.fit_transform(df_processed)
print("PCA transformation successful. Explained Variance Ratio:", pca.explained_variance_ratio_)

# Select the most significant features from PCA components
feature_contributions = pd.DataFrame(
    pca.components_, columns=df_processed.columns, index=[f'PC{i+1}' for i in range(pca.n_components_)]
)
top_features = feature_contributions.abs().sum().sort_values(ascending=False).index[:10]
df_selected = df_processed[top_features]
print("Top 10 PCA-selected features:", list(top_features))

"""PLOT FOR FEATURE SELECTION"""

# Sort features by importance
top_features_contributions = feature_contributions.abs().sum().sort_values(ascending=False)

# Plot the ranked top features
plt.figure(figsize=(10, 5))
sns.barplot(x=top_features_contributions.values[:10], y=top_features_contributions.index[:10], palette="Blues_r")
plt.xlabel("Total Contribution to Principal Components")
plt.ylabel("Features")
plt.title("Top 10 Features Ranked by PCA Contribution")
plt.grid(axis='x')

plt.show()

# Define target variable
y = df['Target']

"""DISTRIBUTION OF TARGET VARIABLE"""

mypal= ['#FC05FB', '#FEAEFE', '#FCD2FC','#F3FEFA', '#B4FFE4','#3FFEBA']

plt.figure(figsize=(7, 5),facecolor='#F6F5F4')
total = float(len(df))
ax = sns.countplot(x=df['Target'], palette=mypal[1::4])
ax.set_facecolor('#F6F5F4')

for p in ax.patches:

    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,height + 3,'{:1.1f} %'.format((height/total)*100), ha="center",
           bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.5))

ax.set_title('Target variable distribution', fontsize=20, y=1.05)
sns.despine(right=True)
sns.despine(offset=5, trim=True)

# Balance dataset using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(df_selected, y)
X_resampled, y_resampled = shuffle(X_resampled, y_resampled, random_state=42)
X_resampled = X_resampled.reset_index(drop=True)
y_resampled = y_resampled.reset_index(drop=True)
print(X_resampled)
print(y_resampled)

print(X_resampled.shape)
print(y_resampled.shape)

"""AFTER BALANCING TARGET VARIABLE DISTRIBUTION PLOT"""

# Define custom color palette
mypal = ['#FC05FB', '#FEAEFE', '#FCD2FC','#F3FEFA', '#B4FFE4','#3FFEBA']

# Create a DataFrame for visualization
df_resampled = pd.DataFrame(X_resampled, columns=df_selected.columns)
df_resampled['Target'] = y_resampled  # Add target column

# Plot settings
plt.figure(figsize=(7, 5), facecolor='#F6F5F4')
total = float(len(df_resampled))
ax = sns.countplot(x=df_resampled['Target'], palette=mypal[1::4])  # Apply custom colors
ax.set_facecolor('#F6F5F4')

# Add percentage labels on bars
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width()/2., height + 3,
            '{:1.1f} %'.format((height/total)*100),
            ha="center", fontsize=12,
            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.5))

# Titles and layout adjustments
ax.set_title('Target Variable Distribution After SMOTE', fontsize=16, y=1.05)
ax.set_xlabel("Target Class", fontsize=12)
ax.set_ylabel("Count", fontsize=12)

sns.despine(right=True)
sns.despine(offset=5, trim=True)

plt.show()

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)
print("Data split into training and testing sets.")
print(X_train.shape,y_train.shape)
print(X_test.shape,y_test.shape)

"""TRAIN INDIVIDUAL MODELS"""

# Train individual models
rf = RandomForestClassifier(n_estimators=100, random_state=42)
dt = DecisionTreeClassifier(random_state=42)
gb = GradientBoostingClassifier(n_estimators=100, random_state=42)

models = {'Random Forest': rf, 'Decision Tree': dt, 'Gradient Boosting': gb}
accuracies = {}

for name, model in models.items():
    scores = cross_val_score(model, X_train, y_train, cv=5)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    accuracies[name] = acc
    print(f"{name} Accuracy: {acc:.4f} (CV Avg: {scores.mean():.4f})")

"""PLOT FOR MODEL ACCURACY COMPARISON"""

# Define custom color palette
mypal = ['#FC05FB', '#FEAEFE', '#FCD2FC', '#F3FEFA', '#B4FFE4', '#3FFEBA']

# Convert accuracies dictionary to DataFrame
accuracy_df = pd.DataFrame(accuracies.items(), columns=['Model', 'Accuracy'])

# Create figure
plt.figure(figsize=(10, 6), facecolor='#F6F5F4')
ax = sns.barplot(data=accuracy_df, x='Model', y='Accuracy', palette=mypal[1::4])

# Add value labels
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width()/2., height + 0.01, f'{height:.4f}', ha="center",
            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.5))

# Formatting
ax.set_ylim(0, 1)
ax.set_ylabel("Accuracy", fontsize=14)
ax.set_xlabel("Model", fontsize=14)
ax.set_title("Model Accuracy Comparison", fontsize=18)
sns.despine()

plt.show()

""" ENSEMBLE MODELS"""

# Voting Classifier
voting_clf = VotingClassifier(estimators=[('rf', rf), ('dt', dt), ('gb', gb)], voting='soft')
voting_clf.fit(X_train, y_train)
y_pred_voting = voting_clf.predict(X_test)
accuracy_voting = accuracy_score(y_test, y_pred_voting)
print(f"\nVoting Classifier Accuracy: {accuracy_voting:.4f}")
precision_voting = precision_score(y_test, y_pred_voting)
recall_voting = recall_score(y_test, y_pred_voting)
f1_voting = f1_score(y_test, y_pred_voting)
roc_auc_voting = roc_auc_score(y_test, y_pred_voting)

print(f"\nðŸ”¹ Voting Classifier Model Performance:")
print(f"âœ… Accuracy: {accuracy_voting:.4f}")
print(f"âœ… Precision: {precision_voting:.4f}")
print(f"âœ… Recall: {recall_voting:.4f}")
print(f"âœ… F1-score: {f1_voting:.4f}")
print(f"Voting Classifier Accuracy: {accuracy_voting:.4f}")

rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Bagging Classifier
bagging_rf = BaggingClassifier(estimator=rf, n_estimators=15, random_state=42)
bagging_rf.fit(X_train, y_train)
y_pred_bagging_rf = bagging_rf.predict(X_test)

# Calculate performance metrics for Bagging Classifier
accuracy_bagging = accuracy_score(y_test, y_pred_bagging_rf)
precision_bagging = precision_score(y_test, y_pred_bagging_rf)
recall_bagging = recall_score(y_test, y_pred_bagging_rf)
f1_bagging = f1_score(y_test, y_pred_bagging_rf)
roc_auc_bagging = roc_auc_score(y_test, y_pred_bagging_rf)

# Print Model Performance
print(f"\nðŸ”¹ Bagging Classifier Model Performance:")
print(f"âœ… Accuracy: {accuracy_bagging:.4f}")
print(f"âœ… Precision: {precision_bagging:.4f}")
print(f"âœ… Recall: {recall_bagging:.4f}")
print(f"âœ… F1-score: {f1_bagging:.4f}")
print(f"âœ… ROC-AUC Score: {roc_auc_bagging:.4f}")

"""PLOT FOR ACCURACY COMPARISON"""

# Define custom color palette
mypal = ['#FC05FB', '#FEAEFE', '#FCD2FC', '#F3FEFA', '#B4FFE4', '#3FFEBA']

# Accuracy dictionary for visualization
accuracy_dict = {
    "Voting Classifier": accuracy_voting,
    "Bagging Classifier": accuracy_bagging
}

# Convert to DataFrame
accuracy_df = pd.DataFrame(accuracy_dict.items(), columns=['Model', 'Accuracy'])

# Create figure
plt.figure(figsize=(8, 5), facecolor='#F6F5F4')
ax = sns.barplot(data=accuracy_df, x='Model', y='Accuracy', palette=mypal[1::4])

# Add value labels
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width()/2., height + 0.01, f'{height:.4f}', ha="center",
            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.5))

# Formatting
ax.set_ylim(0, 1)
ax.set_ylabel("Accuracy", fontsize=14)
ax.set_xlabel("Model", fontsize=14)
ax.set_title("Accuracy Comparison: Voting vs Bagging Classifier", fontsize=16)
sns.despine()

plt.show()

"""PLOT FOR MODEL PERFORMANCE COMPARISON"""

# Define custom color palette
mypal = ['#FC05FB', '#FEAEFE', '#FCD2FC', '#F3FEFA', '#B4FFE4', '#3FFEBA']

# Create a DataFrame for model performance metrics
performance_data = {
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC-AUC'],
    'Voting Classifier': [accuracy_voting, precision_voting, recall_voting, f1_voting, roc_auc_voting],
    'Bagging Classifier': [accuracy_bagging, precision_bagging, recall_bagging, f1_bagging, roc_auc_bagging]
}

performance_df = pd.DataFrame(performance_data)

# Melt dataframe for better visualization
performance_df = performance_df.melt(id_vars="Metric", var_name="Model", value_name="Score")

# Create figure
plt.figure(figsize=(10, 6), facecolor='#F6F5F4')
ax = sns.barplot(data=performance_df, x='Metric', y='Score', hue='Model', palette=mypal[::2])

# Add value labels
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width()/2., height + 0.01, f'{height:.4f}', ha="center",
            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.5))

# Formatting
ax.set_ylim(0, 1)
ax.set_ylabel("Score", fontsize=14)
ax.set_xlabel("Performance Metric", fontsize=14)
ax.set_title("Voting Classifier vs Bagging Classifier Performance", fontsize=18)
sns.despine()

plt.show()

#BEST MODEL PLOT

# Define custom color palette
mypal = ['#FC05FB', '#FEAEFE', '#FCD2FC', '#F3FEFA', '#B4FFE4', '#3FFEBA']

# Store model performance
model_scores = {
    "Voting Classifier": accuracy_voting,
    "Bagging Classifier": accuracy_bagging
}

# Find the best model based on accuracy
best_model_name = max(model_scores, key=model_scores.get)

# Store its performance metrics
if best_model_name == "Voting Classifier":
    best_metrics = {
        "Accuracy": accuracy_voting,
        "Precision": precision_voting,
        "Recall": recall_voting,
        "F1-score": f1_voting,
        "ROC-AUC": roc_auc_voting
    }
else:
    best_metrics = {
        "Accuracy": accuracy_bagging,
        "Precision": precision_bagging,
        "Recall": recall_bagging,
        "F1-score": f1_bagging,
        "ROC-AUC": roc_auc_bagging
    }

# Convert to DataFrame
best_model_df = pd.DataFrame(best_metrics.items(), columns=['Metric', 'Score'])

# Plot
plt.figure(figsize=(8, 5), facecolor='#F6F5F4')
ax = sns.barplot(data=best_model_df, x='Metric', y='Score', palette=mypal[::2])

# Add value labels
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width()/2., height + 0.01, f'{height:.4f}', ha="center",
            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.5))

# Formatting
ax.set_ylim(0, 1)
ax.set_ylabel("Score", fontsize=14)
ax.set_xlabel("Performance Metric", fontsize=14)
ax.set_title(f"Performance of Best Model: {best_model_name}", fontsize=18)
sns.despine()

plt.show()



